{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14755296,"sourceType":"datasetVersion","datasetId":9430873},{"sourceId":14755553,"sourceType":"datasetVersion","datasetId":9431060},{"sourceId":14755856,"sourceType":"datasetVersion","datasetId":9431291},{"sourceId":295717479,"sourceType":"kernelVersion"}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys, subprocess\n\ndef pip_install(pkgs):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkgs)\n\npip_install([\n    \"ultralytics\",\n    \"opencv-python\",\n    \"torchvision\",\n    \"numpy\",\n    \"pandas\"\n])\n\nimport os, cv2, json, time\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nfrom dataclasses import dataclass, asdict\nfrom typing import Dict, List, Tuple\n\nfrom ultralytics import YOLO\nfrom torchvision.models.segmentation import deeplabv3_resnet50\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====== PATHS ======\nVIDEO_IN_1 = \"/kaggle/input/demo_video/demo_vid.mov\"\nVIDEO_IN_2 = \"/kaggle/input/demo_video/demo_vid2.mov\"\n\nYOLO_W = \"/kaggle/input/yolo-bdd100k-best/best.pt\"\nDEEPLAB_W = \"/kaggle/input/deeplabv3-bdd100k-best/best.pt\"\n\nOUT_DIR = \"/kaggle/working/outputs\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\n# ====== CLASS DEFINITIONS ======\nYOLO_CLASSES = [\"car\", \"person\", \"truck\", \"bus\", \"traffic_control\"]\n\nSEG_OTHER = 0\nSEG_ROAD = 1\nSEG_SIDEWALK = 2\nSEG_PERSON = 3\nSEG_RIDER = 4\nSEG_VEHICLE = 5\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@dataclass\nclass Config:\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # YOLO\n    yolo_imgsz: int = 768\n    yolo_conf: float = 0.25\n    yolo_iou: float = 0.50\n    yolo_tracker: str = \"bytetrack.yaml\"\n\n    # DeepLab\n    seg_in_size: Tuple[int,int] = (512, 512)\n    seg_every_n: int = 5          # không cần chạy mỗi frame\n    morph_kernel: int = 7         # làm mịn mask\n\n    # Rule thresholds\n    person_road_thr: float = 0.25\n    vehicle_sidewalk_thr: float = 0.20\n\n    # Visualization\n    bottom_roi_start: float = 0.6\n    seg_alpha: float = 0.35\n\ncfg = Config()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DeepLab6:\n    def __init__(self, ckpt_path: str, device: str):\n        self.device = device\n        self.model = deeplabv3_resnet50(weights=None, num_classes=6)\n\n        ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n        if \"state_dict\" in ckpt:\n            ckpt = ckpt[\"state_dict\"]\n\n        ckpt = {k.replace(\"module.\", \"\"): v for k, v in ckpt.items()}\n        self.model.load_state_dict(ckpt, strict=False)\n\n        self.model.to(self.device).eval()\n\n    @torch.inference_mode()\n    def infer(self, frame_bgr: np.ndarray, size_hw: Tuple[int,int]):\n        H, W = frame_bgr.shape[:2]\n        h, w = size_hw\n\n        img = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, (w, h))\n        x = torch.from_numpy(img).permute(2,0,1).float() / 255.0\n        x = x.unsqueeze(0).to(self.device)\n\n        out = self.model(x)[\"out\"]\n        seg = out.argmax(1)\n\n        seg = F.interpolate(\n            seg.unsqueeze(1).float(),\n            size=(H, W),\n            mode=\"nearest\"\n        ).squeeze(1).long()\n\n        return seg[0].cpu().numpy()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def bbox_area(x1,y1,x2,y2):\n    return max(0, x2-x1) * max(0, y2-y1)\n\ndef lower_bbox(x1,y1,x2,y2, frac=0.33):\n    h = y2 - y1\n    return x1, int(y2 - h*frac), x2, y2\n\ndef intersection_ratio(mask: np.ndarray, box):\n    x1,y1,x2,y2 = box\n    area = bbox_area(x1,y1,x2,y2)\n    if area == 0:\n        return 0.0\n    return np.count_nonzero(mask[y1:y2, x1:x2]) / area\n\ndef smooth_mask(mask: np.ndarray, k: int):\n    kernel = np.ones((k,k), np.uint8)\n    m = (mask.astype(np.uint8) * 255)\n    m = cv2.morphologyEx(m, cv2.MORPH_OPEN, kernel)\n    m = cv2.morphologyEx(m, cv2.MORPH_CLOSE, kernel)\n    return m > 0\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_video(video_in, video_out, log_path):\n    yolo = YOLO(YOLO_W)\n    deeplab = DeepLab6(DEEPLAB_W, cfg.device)\n\n    cap = cv2.VideoCapture(video_in)\n    fps = cap.get(cv2.CAP_PROP_FPS) or 30\n    W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    writer = cv2.VideoWriter(\n        video_out,\n        cv2.VideoWriter_fourcc(*\"mp4v\"),\n        fps,\n        (W, H)\n    )\n\n    log_f = open(log_path, \"w\", encoding=\"utf-8\")\n\n    seg_cache = np.zeros((H, W), dtype=np.int32)\n\n    frame_idx = 0\n    while True:\n        ok, frame = cap.read()\n        if not ok:\n            break\n\n        # --- segmentation mỗi N frame\n        if frame_idx % cfg.seg_every_n == 0:\n            seg_cache = deeplab.infer(frame, cfg.seg_in_size)\n\n        road_mask = smooth_mask(seg_cache == SEG_ROAD, cfg.morph_kernel)\n        sidewalk_mask = smooth_mask(seg_cache == SEG_SIDEWALK, cfg.morph_kernel)\n\n        out = frame.copy()\n\n        # --- overlay DeepLab (bottom ROI)\n        y0 = int(H * cfg.bottom_roi_start)\n        heat = np.maximum(\n            road_mask[y0:]*255,\n            sidewalk_mask[y0:]*180\n        ).astype(np.uint8)\n\n        heat_color = cv2.applyColorMap(heat, cv2.COLORMAP_JET)\n        out[y0:] = cv2.addWeighted(\n            out[y0:], 1-cfg.seg_alpha,\n            heat_color, cfg.seg_alpha, 0\n        )\n\n        # --- YOLO detection + tracking\n        res = yolo.track(\n            source=frame,\n            imgsz=cfg.yolo_imgsz,\n            conf=cfg.yolo_conf,\n            iou=cfg.yolo_iou,\n            persist=True,\n            tracker=cfg.yolo_tracker,\n            verbose=False\n        )[0]\n\n        if res.boxes is not None:\n            for box, cls_id, conf, tid in zip(\n                res.boxes.xyxy.cpu().numpy(),\n                res.boxes.cls.cpu().numpy().astype(int),\n                res.boxes.conf.cpu().numpy(),\n                res.boxes.id.cpu().numpy() if res.boxes.id is not None else [-1]*len(res.boxes)\n            ):\n                x1,y1,x2,y2 = map(int, box)\n                cls = YOLO_CLASSES[cls_id]\n\n                bx = lower_bbox(x1,y1,x2,y2)\n\n                # rule examples\n                if cls == \"person\":\n                    r = intersection_ratio(road_mask, bx)\n                    if r > cfg.person_road_thr:\n                        log_f.write(json.dumps({\n                            \"frame\": frame_idx,\n                            \"track_id\": int(tid),\n                            \"event\": \"PedestrianOnRoad\",\n                            \"ratio\": r\n                        }) + \"\\n\")\n\n                if cls in [\"car\",\"truck\",\"bus\"]:\n                    r = intersection_ratio(sidewalk_mask, bx)\n                    if r > cfg.vehicle_sidewalk_thr:\n                        log_f.write(json.dumps({\n                            \"frame\": frame_idx,\n                            \"track_id\": int(tid),\n                            \"event\": \"VehicleOnSidewalk\",\n                            \"ratio\": r\n                        }) + \"\\n\")\n\n                cv2.rectangle(out, (x1,y1), (x2,y2), (0,255,0), 2)\n                cv2.putText(\n                    out,\n                    f\"{cls}#{int(tid)}\",\n                    (x1, y1-5),\n                    cv2.FONT_HERSHEY_SIMPLEX,\n                    0.6,\n                    (255,255,255),\n                    2\n                )\n\n        writer.write(out)\n        frame_idx += 1\n\n    log_f.close()\n    writer.release()\n    cap.release()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"run_video(\n    VIDEO_IN_1,\n    f\"{OUT_DIR}/demo1_out.mp4\",\n    f\"{OUT_DIR}/demo1_events.jsonl\"\n)\n\nrun_video(\n    VIDEO_IN_2,\n    f\"{OUT_DIR}/demo2_out.mp4\",\n    f\"{OUT_DIR}/demo2_events.jsonl\"\n)\n\nprint(\"DONE\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}